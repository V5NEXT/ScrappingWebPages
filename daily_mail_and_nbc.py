# -*- coding: utf-8 -*-
"""daily_mail and NBC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mt9Lo5KApXqCdNN_57OCej3pT9_vVBRP
"""

#importing requests 
import requests
#importing BeautifulSoup for scrapping templates
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd
import re
import time



# Daily Mail
url = "https://www.dailymail.co.uk"

# Request
r1 = requests.get(url)
r1.status_code

# We'll save in coverpage the cover page content
coverpage = r1.content

# Soup creation
soup1 = BeautifulSoup(coverpage, 'html5lib')


# News identification
coverpage_news = soup1.find_all('h2', class_='linkro-darkred')
dailymailylen = len(coverpage_news)

#CNN

#tried to access CNN but it didnt had enough news to follow on

url = "https://edition.cnn.com/"
# Request


r1 = requests.get(url)
r1.status_code

# We'll save in coverpage the cover page content
coverpage = r1.content
from bs4 import BeautifulSoup as soup

import requests

from datetime import date
today = date.today()
d = today.strftime("%m-%d-%y")
cnn_url="https://edition.cnn.com/world".format(d)
html = requests.get(cnn_url)
bsobj = soup(html.content,'lxml')
for link in bsobj.findAll("h3"):
    print("Headline : {}".format(link.text))

from bs4 import BeautifulSoup as soup
import requests


nbc_url='https://www.nbcnews.com/politics'
r = requests.get('https://www.nbcnews.com/politics')
b = soup(r.content,'lxml')
politics_news = []
for news in b.findAll('div', {'class': 'wide-tease-item__description'}):
  politics_news.append(news.text.strip())
  df_politics = pd.DataFrame(
     {'Article Content': politics_news 
  })


nbc_url='https://www.nbcnews.com/health/coronavirus'
r = requests.get('https://www.nbcnews.com/health/coronavirus')
b = soup(r.content,'lxml')
for news in b.findAll('div', {'class': 'wide-tease-item__description'}):
  politics_news.append(news.text.strip())
  df_politics = pd.DataFrame(
     {'Article Content': politics_news 
  })


nbc_url='https://www.nbcnews.com/world'
r = requests.get('https://www.nbcnews.com/world')
b = soup(r.content,'lxml')
for news in b.findAll('div', {'class': 'wide-tease-item__description'}):
  politics_news.append(news.text.strip())
  df_politics = pd.DataFrame(
     {'Article Content': politics_news 
  })


nbc_url='https://www.nbcnews.com/business'
r = requests.get('https://www.nbcnews.com/business')
b = soup(r.content,'lxml')
for news in b.findAll('div', {'class': 'wide-tease-item__description'}):
  politics_news.append(news.text.strip())
  df_politics = pd.DataFrame(
     {'Article Content': politics_news 
  })


nbc_url='https://www.nbcnews.com/podcasts'
r = requests.get('https://www.nbcnews.com/podcasts')
b = soup(r.content,'lxml')
for news in b.findAll('div', {'class': 'text-summary'}):
  politics_news.append(news.text.strip())
  df_politics = pd.DataFrame(
     {'Article Content': politics_news 
  })

nbc_url='https://www.nbcnews.com/us-news'
r = requests.get('https://www.nbcnews.com/us-news')
b = soup(r.content,'lxml')
for news in b.findAll('div', {'class': 'wide-tease-item__description'}):
  politics_news.append(news.text.strip())
  df_politics = pd.DataFrame(
     {'Article Content': politics_news 
  })



df_politics

from google.colab import files
df_politics.to_csv('nbcnews.csv') 
files.download('nbcnews.csv')

number_of_articles = dailymailylen

# Empty lists for content, links and titles
news_contents = []
list_links = []
list_titles = []

for n in np.arange(0, number_of_articles):
        
    # Getting the link of the article
    link = url + coverpage_news[n].find('a')['href']
    list_links.append(link)
    
    # Getting the title
    title = coverpage_news[n].find('a').get_text()
    list_titles.append(title)
    
    # Reading the content (it is divided in paragraphs)
    article = requests.get(link)
    article_content = article.content
    soup_article = BeautifulSoup(article_content, 'html5lib')
    body = soup_article.find_all('p', class_='mol-para-with-font')
    
    # Unifying the paragraphs
    list_paragraphs = []
    for p in np.arange(0, len(body)):
        paragraph = body[p].get_text()
        list_paragraphs.append(paragraph)
        final_article = " ".join(list_paragraphs)
        
    # Removing special characters
    final_article = re.sub("\\xa0", "", final_article)
        
    news_contents.append(final_article)

# df_features
df_features = pd.DataFrame(
     {'Article Content': news_contents 
    })

# df_show_info
df_show_info = pd.DataFrame(
    {'Article Title': list_titles,
     'Article Link': list_links})

df_features

df_show_info

from google.colab import files
df_features.to_csv('filename.csv') 
files.download('filename.csv')